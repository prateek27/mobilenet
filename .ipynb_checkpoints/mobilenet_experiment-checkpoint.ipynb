{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datagenerator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Paper link - https://arxiv.org/pdf/1704.04861.pdf\n",
    "\"\"\"\n",
    "from keras.layers import ZeroPadding2D,Conv2D,BatchNormalization,Input,Dropout,DepthwiseConv2D,Input\n",
    "from keras.layers import ReLU,GlobalAveragePooling2D,GlobalMaxPool2D,Reshape,Activation\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "def MobileNet(\n",
    "    input_shape=(224,224,3),\n",
    "    width_multiplier = 1.0, #changes number of filters\n",
    "    depth_multiplier = 1, # Resolution Multiplier\n",
    "    include_top = True,\n",
    "    weights = None,\n",
    "    dropout = 1e-3,\n",
    "    input_tensor = None,\n",
    "    pooling = None, #Global Average/Max Pooling or None\n",
    "    classes = 1000,\n",
    "    ):\n",
    "\n",
    "    #Input Shape\n",
    "    if input_shape is None:\n",
    "        default_size = 224\n",
    "    else:\n",
    "        rows,cols = input_shape[0],input_shape[1]\n",
    "        #Make sure we use one of the mentioned sizes\n",
    "        if rows==cols and rows in [128,160,192,224]:\n",
    "            default_size = rows\n",
    "        else:\n",
    "            default_size = 224\n",
    "\n",
    "    #Input Tensor\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        img_input = input_tensor \n",
    "\n",
    "    #Standard Convolution\n",
    "    #Block 0\n",
    "    x = conv_block(img_input,32,width_multiplier,strides=(2,2))\n",
    "\n",
    "    #Block 1\n",
    "    x = depthwise_conv_block(x,64,width_multiplier,depth_multiplier,block_id=1)\n",
    "    #Block2\n",
    "    x = depthwise_conv_block(x,128,width_multiplier,depth_multiplier,strides=(2,2),block_id=2)\n",
    "    #Block 3\n",
    "    x = depthwise_conv_block(x,128,width_multiplier,depth_multiplier,block_id=3)\n",
    "    #Block 4\n",
    "    x = depthwise_conv_block(x,256,width_multiplier,depth_multiplier,strides=(2,2),block_id=4)\n",
    "    #Block 5\n",
    "    x = depthwise_conv_block(x,256,width_multiplier,depth_multiplier,block_id=5)\n",
    "    #Block 6\n",
    "    x = depthwise_conv_block(x,512,width_multiplier,depth_multiplier,strides=(2,2),block_id=6)\n",
    "    #Block 7\n",
    "    x = depthwise_conv_block(x,512,width_multiplier,depth_multiplier,block_id=7)\n",
    "    #Block 8\n",
    "    x = depthwise_conv_block(x,512,width_multiplier,depth_multiplier,block_id=8)\n",
    "    #Block 9\n",
    "    x = depthwise_conv_block(x,512,width_multiplier,depth_multiplier,block_id=9)\n",
    "    #Block 10\n",
    "    x = depthwise_conv_block(x,512,width_multiplier,depth_multiplier,block_id=10)\n",
    "    #Block 11\n",
    "    x = depthwise_conv_block(x,512,width_multiplier,depth_multiplier,block_id=11)\n",
    "    #Block 12\n",
    "    x = depthwise_conv_block(x,1024,width_multiplier,depth_multiplier,strides=(2,2),block_id=12)\n",
    "    #Block 13\n",
    "    x = depthwise_conv_block(x,1024,width_multiplier,depth_multiplier,block_id=13)\n",
    "\n",
    "    if include_top:\n",
    "        shape = (1,1,int(1024*width_multiplier))\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Reshape(shape,name='reshape_1')(x)\n",
    "        x = Dropout(dropout,name='dropout')(x)\n",
    "        x = Conv2D(classes,(1,1),padding='same',name='conv_fc')(x)\n",
    "        x = Activation('softmax',name='softmax')(x)\n",
    "        x = Reshape((classes,),name='reshape_2')(x)\n",
    "    else:\n",
    "        if pooling=='avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling=='max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    #Create Model using functional API\n",
    "    model = Model(inputs=img_input,outputs=x,name='mobilenet')\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(inputs,filters,width_multiplier,kernel_size=(3,3),strides=(1,1)):\n",
    "    #Inital Conv Layer with Batch Norm and Relu6\n",
    "    #Input Shape : 4D Tensor (Samples,Rows,Cols,Channels)\n",
    "    #Output Shape : 4D Tensor(Samples,New_Rows,New_Cols,Channels)\n",
    "    #Width_Multiplier : Changes Number of Filters\n",
    "\n",
    "    filters = int(filters*width_multiplier)\n",
    "\n",
    "    #Params in Zero Padding ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "    x = ZeroPadding2D(padding=((0,1),(0,1)),name='conv1_pad')(inputs)\n",
    "\n",
    "    #Apply Standard Convolution without Bias and Batch Norm\n",
    "    x = Conv2D(filters,kernel_size,padding='valid',use_bias=False,strides=strides,name='conv1')(x)\n",
    "    x = BatchNormalization(name='conv1_bn')(x)\n",
    "\n",
    "\n",
    "    # ReLu6 = min(max(features, 0), 6)\n",
    "    x = ReLU(6.,name='conv1_relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def depthwise_conv_block(inputs,pointwise_conv_filters,width_multiplier,depth_multiplier=1,strides=(1,1),block_id=1):\n",
    "\n",
    "    \"\"\"\n",
    "    depth_multiplier: The number of depthwise convolution output channels\n",
    "            for each input channel.\n",
    "    The total number of depthwise convolution output\n",
    "    channels will be equal to `filters_in * depth_multiplier`.\n",
    "    \"\"\"\n",
    "\n",
    "    #Update the Number of Output Filters\n",
    "    pointwise_conv_filters = int(pointwise_conv_filters*width_multiplier)\n",
    "\n",
    "    if strides==(1,1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = ZeroPadding2D(padding=((0,1),(0,1)),name='conv_pad_%d'%block_id)(inputs)\n",
    "\n",
    "    # Depth Wise Convolution\n",
    "    x = DepthwiseConv2D((3,3),padding='same' if strides==(1,1) else 'valid',depth_multiplier=depth_multiplier,strides=strides,use_bias=False,name='conv_dw_%d'%block_id)(x)\n",
    "    x = BatchNormalization(name='conv_dw_%d_bn'%block_id)(x)\n",
    "    x = ReLU(6.,name='conv_dw_%d_relu'%block_id)(x)\n",
    "\n",
    "    # PointWise Convolution with 1X1 Filters, No of Filters = pointwise_conv_filters\t\n",
    "    x = Conv2D(pointwise_conv_filters,(1,1),padding='same',use_bias=False,strides=(1,1),name='conv_pw_%d'%block_id)(x)\n",
    "    x = BatchNormalization(name='conv_pw_%d_bn'%block_id)(x)\n",
    "    x = ReLU(6.,name='conv_pw_%d_relu'%block_id)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNet(width_multiplier=1,depth_multiplier=1,classes=NO_OF_CLASSES)\n",
    "\n",
    "model.compile(optimizer='adam',metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = DataGenerator(batch_size=1,dim=(224,224),n_channels=3,list_IDs=partition['train'],\n",
    "                                labels=labels,n_classes=NO_OF_CLASSES)\n",
    "\n",
    "val_generator = DataGenerator(batch_size=1,dim=(224,224),n_channels=3,list_IDs=partition['train'],\n",
    "                                labels=labels,n_classes=NO_OF_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.Image' has no attribute 'resize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3f389a2bcf09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/home/prateek/Prateek/Object_Classification/mobilenet/datagenerator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_IDs_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/prateek/Prateek/Object_Classification/mobilenet/datagenerator.py\u001b[0m in \u001b[0;36m__data_generation\u001b[0;34m(self, list_IDs_temp)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m#img = Image.open(ID)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'resize'"
     ]
    }
   ],
   "source": [
    "train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = ModelCheckpoint(filepath='weights/{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_acc', save_best_only=True)\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=1, batch_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs = 100, callbacks = [ckpt,tb ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_applications import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model = mobilenet.MobileNet(input_shape=(224,224,3), alpha=1.0, depth_multiplier=1, \n",
    "                              dropout=1e-3, include_top=True, weights=None, \n",
    "                              input_tensor=None, pooling=None, classes=NO_OF_CLASSES)\n",
    "k_model.compile(optimizer='adam',metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model.fit_generator(train_generator, validation_data=val_generator, pickle_safe=False, epochs=100, max_queue_size=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "inner_loop = len( partition['train'] )//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print (\"Epoch \" + str(epoch+1) + \" Running\")\n",
    "    for ind in range(inner_loop):\n",
    "        X,y = train_generator.__getitem__(ind)\n",
    "        k_model.fit(X, y, epochs=1, batch_size=batch_size, verbose=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
