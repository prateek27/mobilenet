{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "from keras.utils import Sequence\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../tiny-imagenet-200/train'\n",
    "#Process Validation Data\n",
    "base_path_valid = '../tiny-imagenet-200/val'\n",
    "st = '../tiny-imagenet-200/val/images/'\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "IMG_DIM = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_input(x,mode='tf'):\n",
    "        \"\"\"\n",
    "            - tf: will scale pixels between -1 and 1,\n",
    "                sample-wise.\n",
    "            - torch: will scale pixels between 0 and 1 and then\n",
    "                will normalize each channel with respect to the\n",
    "                ImageNet dataset.\n",
    "                \n",
    "        \"\"\"\n",
    "        if mode=='tf':\n",
    "            x = x/127.5\n",
    "            x -= 1\n",
    "            return x\n",
    "        \n",
    "        if mode=='torch':\n",
    "            mean = [0.485,0.456,0.406]\n",
    "            std = [0.229,0.224,0.225]\n",
    "            x /= 255.0\n",
    "            x[...,0] -= mean[0]\n",
    "            x[...,1] -= mean[1]\n",
    "            x[...,2] -= mean[2]\n",
    "            \n",
    "            x[...,0] /= std[0]\n",
    "            x[...,1] /= std[1]\n",
    "            x[...,2] /= std[2]\n",
    "            \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=None, n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, f in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            img = Image.open(f)\n",
    "            img = img.resize(self.dim)\n",
    "            img = img.convert('RGB')\n",
    "            X[i,] = preprocess_input(np.array(img,dtype=np.float32))\n",
    "            img.close()\n",
    "            \n",
    "            \n",
    "            # Store class\n",
    "            y[i] = self.labels[f]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {\n",
    "    'train': [],\n",
    "    'validation': []\n",
    "}\n",
    "labels = {}\n",
    "class_ids = {}\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in os.listdir(base_path):\n",
    "    temp = os.path.join(base_path, label)\n",
    "    \n",
    "    if class_ids.get(label) is None:\n",
    "        class_ids[label] = cnt\n",
    "        cnt += 1\n",
    "    \n",
    "    img_fldr_path = os.path.join(temp, 'images')\n",
    "    for imgs in os.listdir(img_fldr_path):\n",
    "        ID = os.path.join(img_fldr_path, imgs)\n",
    "        partition['train'].append( ID )\n",
    "        labels[ID] = class_ids[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(sorted(class_ids.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_CLASSES = len((os.listdir(base_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(base_path_valid,\"val_annotations.txt\")) as f:\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        img_name = tokens[0]\n",
    "        img_label = tokens[1]\n",
    "        ID = os.path.join(st,img_name)\n",
    "        partition['validation'].append(ID)\n",
    "        labels[ID] = class_ids[img_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = DataGenerator(batch_size=128,dim=(224,224),n_channels=3,list_IDs=partition['train'],\n",
    "                                labels=labels,n_classes=NO_OF_CLASSES)\n",
    "\n",
    "val_generator = DataGenerator(batch_size=128,dim=(224,224),n_channels=3,list_IDs=partition['train'],\n",
    "                                labels=labels,n_classes=NO_OF_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 82 105   0  73  93 104 180 107 133 124 196 144  21 133 141  91 102  61\n",
      "  40  22 101  10  48 136 164  17  51 103  13  31 160 126  31  99  33  25\n",
      "  19 144  79 199 122  37  55  39   1  42  37  66 185 120  86 171 170  22\n",
      "  46  20   7  92  84  14  35   1 171 178 114  62 148 133  87  84  23 107\n",
      "  20 140 192 105  33   5  94  77 143 157  64 101  99  89  74  95 199 102\n",
      "  75  68 169  33   8  43 136 110 123  68  76 110 151 198  24 119 137 126\n",
      "  11  64 169  29  33 198  48  25 189 197  85 114 122  86  32 124 124 125\n",
      "  27 185]\n",
      "(128, 224, 224, 3) (128, 200)\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y) in enumerate(train_generator):\n",
    "    #print(np.argmax(y,axis=1))\n",
    "    print(np.argmax(y,axis=-1))\n",
    "    print(x.shape,y.shape)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
